 for i, det in enumerate(pred):  # Detections per image
        if len(det):
            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()

            # Iterate over detections and draw bounding boxes
            for *xyxy, conf, cls in reversed(det):
                c = int(cls)  # Integer class
                label = f'{names[c]} {conf:.2f}'
                color = color_list[c]

                # Draw bounding boxes
                cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), color, 2)
                cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)

                # Convert the frame to grayscale and equalize the histogram
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                gray = cv2.equalizeHist(gray)

                # Detect faces in the grayscale frame
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30),
                                                      flags=cv2.CASCADE_SCALE_IMAGE)

                # Iterate over the detected faces
                for (x, y, w, h) in faces:
                    # Compute the center of the face
                    center = (x + w // 2, y + h // 2)

                    # Update the points queue
                    pts.appendleft(center)

                    # Draw a rectangle around the face
                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

                    # Draw the center of the face
                    cv2.circle(frame, center, 5, (0, 0, 255), -1)

                    # Calculate the distance between the center of the face and the color
                    color_name = names[c]
                    color_center = (int((xyxy[0] + xyxy[2]) / 2), int((xyxy[1] + xyxy[3]) / 2))
                    for color, bounds in colors.items():
                        lower = bounds['lower']
                        upper = bounds['upper']
                        if lower[0] <= hsv[color_center[1], color_center[0], 0] <= upper[0]:
                            distance = calculate_distance(center, color_center)

                            # Draw a line between the face and the color
                            cv2.line(frame, center, color_center, color_labels[color], 2)

                            # Check if the distance is less than 100
                            if distance < 100:
                                # Play a sound
                                winsound.Beep(1000, 500)  # Beep with frequency 1000Hz for 500 milliseconds

                            # Label the color in the stream
                            cv2.putText(frame, color_name, (int(xyxy[0]), int(xyxy[1]) + 40), cv2.FONT_HERSHEY_SIMPLEX,
                                        0.9, color_labels[color], 2)

                    # Detect face expression
                    # Perform emotion analysis on the frame
                    result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)

                    # Retrieve the dominant emotion from the first result in the list
                    dominant_emotion = result[0]['dominant_emotion']

                    # Display the dominant emotion label on the frame
                    cv2.putText(
                        frame,
                        dominant_emotion,
                        (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.9,
                        (0, 255, 0),
                        2
                    )

    # Show the output frame
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1) & 0xFF

    # If the 'q' key is pressed, break from the loop
    if key == ord("q"):
        break

# Release the video stream and close any open windows
video_capture.release()
cv2.destroyAllWindows()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556578ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\Users\\dell\\HCI_Project_Phase_2\\yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd C:\\Users\\dell\\HCI_Project_Phase_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e563ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu==2.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffe831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef07156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install dlib-19.22.99-cp310-cp310-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow-GPU enabled:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bf95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334775ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b99ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # GPU(s) detected\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU:\", gpu)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9010f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from torchvision.transforms import functional as F\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import face_recognition\n",
    "from collections import deque\n",
    "from imutils.video import VideoStream\n",
    "import imutils\n",
    "import time\n",
    "from gaze_tracking import GazeTracking\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import face_recognition\n",
    "import deepface\n",
    "from deepface import DeepFace\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "from PIL import Image\n",
    "from playsound import playsound\n",
    "import winsound\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0069706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_filter = ['train'] #You can give list of classes to filter by name ['train','person' ]\n",
    "\n",
    "\n",
    "opt  = {\n",
    "    \"weights\": \"weights/yolov7.pt\", # Path to weights file default weights are for nano model\n",
    "    \"yaml\"   : \"data/coco.yaml\",\n",
    "    \"img-size\": 640, # default image size\n",
    "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
    "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
    "    \"device\" : 'cpu',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
    "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\moham\\\\Downloads\\\\HCI_Project-main\\\\HCI_Project-main\\\\Users.txt\"  # Replace with the actual file path\n",
    "# Initialize the emotion analysis\n",
    "known_face_names = []  # Replace with your known face names\n",
    "known_face_encodings = []\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = line.strip().split(\",\")\n",
    "        # data will be a list containing the split values\n",
    "        \n",
    "        # Access individual values\n",
    "        image_path = data[0]\n",
    "        name = data[1]\n",
    "        code = data[2]\n",
    "        \n",
    "        image = face_recognition.load_image_file(image_path)\n",
    "        face_encoding = face_recognition.face_encodings(image)[0]  # Assuming there's only one face in each image\n",
    "        known_face_encodings.append(face_encoding)\n",
    "        known_face_names.append(name)\n",
    "        # Do something with the values\n",
    "        print(\"Image Path:\", image_path)\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Code:\", code)\n",
    "        \n",
    "        # Additional processing...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2406a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_path  # Replace with the actual file path\n",
    "data_list = []\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        data = line.strip().split(\",\")\n",
    "        data_list.append(data)\n",
    "\n",
    "# Print the list\n",
    "for row in data_list:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d984ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the lower and upper boundaries of the colors in the HSV color space\n",
    "colors = {\n",
    "    'blue': {\n",
    "        'lower': np.array([90, 100, 100]),\n",
    "        'upper': np.array([130, 255, 255])\n",
    "    },\n",
    "    'yellow': {\n",
    "        'lower': np.array([20, 100, 100]),\n",
    "        'upper': np.array([30, 255, 255])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Color labels for drawing text on the frame\n",
    "color_labels = {\n",
    "    'blue': (255, 0, 0),\n",
    "    'yellow': (0, 255, 255)\n",
    "}\n",
    "\n",
    "# Initialize the list of tracked points, the frame counter, and the coordinate deltas\n",
    "pts = deque(maxlen=32)\n",
    "counter = 0\n",
    "(dX, dY) = (0, 0)\n",
    "direction = \"\"\n",
    "\n",
    "# Load the face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load object detection model\n",
    "weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "# Set device and half precision\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "half = device.type != 'cpu'\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights, map_location=device)\n",
    "stride = int(model.stride.max())\n",
    "imgsz = check_img_size(imgsz, s=stride)  # Check image size\n",
    "\n",
    "if half:\n",
    "    model.half()  # To FP16\n",
    "\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "color_list = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "gaze = GazeTracking()\n",
    "# Initialize a timer and a variable to track the duration of closed eyes\n",
    "closed_eye_timer = None\n",
    "closed_eye_duration = 0\n",
    "sleep_threshold = 7  # Number of seconds to consider as sleeping\n",
    "video_capture = cv2.VideoCapture(0)  # 0 represents the default camera\n",
    "# cv2.namedWindow(\"Real-time Emotion Analysis\")\n",
    "\n",
    "# Allow the camera to warm up\n",
    "time.sleep(2.0)\n",
    "\n",
    "# Calculate the distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "while True:\n",
    "    # Read the next frame from the video stream\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Face detection\n",
    "    # Load face recognition model\n",
    "    for data in data_list:\n",
    "        image_path = data[0]\n",
    "        name = data[1]\n",
    "        Password = data[2]\n",
    "    \n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "    \n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "\n",
    "        face_names.append(name)\n",
    "\n",
    "        # Display the Face Detection results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "    \n",
    "    # Gaze Tracking\n",
    "    gaze.refresh(frame)\n",
    "    new_frame = gaze.annotated_frame()\n",
    "    text = \"\"\n",
    "    if gaze.is_right():\n",
    "        text = \"Looking right\"\n",
    "    elif gaze.is_left():\n",
    "        text = \"Looking left\"\n",
    "    elif gaze.is_center():\n",
    "        text = \"Looking center\"\n",
    "    elif gaze.is_blinking():\n",
    "        if closed_eye_timer is None:\n",
    "            # Start the timer if the eyes are closed for the first time\n",
    "            closed_eye_timer = time.time()\n",
    "        else:\n",
    "            # Calculate the duration of closed eyes\n",
    "            closed_eye_duration = time.time() - closed_eye_timer\n",
    "\n",
    "            if closed_eye_duration >= sleep_threshold:\n",
    "                text = \"He is sleeping\"\n",
    "            else:\n",
    "                text = \"Blinking\"\n",
    "    else:\n",
    "        # Reset the timer if the eyes are open\n",
    "        closed_eye_timer = None\n",
    "        closed_eye_duration = 0\n",
    "\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Resize the frame, blur it, and convert it to the HSV color space\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    blurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "    hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Detect objects using YOLOv7\n",
    "    img = letterbox(frame, imgsz, stride=stride)[0]\n",
    "    img = img.transpose((2, 0, 1))[::-1]\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    t1 = time_synchronized()\n",
    "    pred = model(img, augment=False)[0]\n",
    "    pred = non_max_suppression(pred, 0.4, 0.5, classes=None, agnostic=False)\n",
    "    t2 = time_synchronized()\n",
    "\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # Detections per image\n",
    "        if len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "\n",
    "            # Iterate over detections and draw bounding boxes\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                c = int(cls)  # Integer class\n",
    "                label = f'{names[c]} {conf:.2f}'\n",
    "                color = color_list[c]\n",
    "\n",
    "                # Draw bounding boxes\n",
    "                cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), color, 2)\n",
    "                cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "                # Convert the frame to grayscale and equalize the histogram\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                gray = cv2.equalizeHist(gray)\n",
    "\n",
    "                # Detect faces in the grayscale frame\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30),\n",
    "                                                      flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "                # Iterate over the detected faces\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Compute the center of the face\n",
    "                    center = (x + w // 2, y + h // 2)\n",
    "\n",
    "                    # Update the points queue\n",
    "                    pts.appendleft(center)\n",
    "\n",
    "                    # Draw a rectangle around the face\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                    # Draw the center of the face\n",
    "                    cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "\n",
    "                    # Calculate the distance between the center of the face and the color\n",
    "                    color_name = names[c]\n",
    "                    color_center = (int((xyxy[0] + xyxy[2]) / 2), int((xyxy[1] + xyxy[3]) / 2))\n",
    "                    for color, bounds in colors.items():\n",
    "                        lower = bounds['lower']\n",
    "                        upper = bounds['upper']\n",
    "                        if lower[0] <= hsv[color_center[1], color_center[0], 0] <= upper[0]:\n",
    "                            distance = calculate_distance(center, color_center)\n",
    "\n",
    "                            # Draw a line between the face and the color\n",
    "                            cv2.line(frame, center, color_center, color_labels[color], 2)\n",
    "\n",
    "                            # Check if the distance is less than 100\n",
    "                            if distance < 100:\n",
    "                                # Play a sound\n",
    "                                winsound.Beep(1000, 500)  # Beep with frequency 1000Hz for 500 milliseconds\n",
    "\n",
    "                            # Label the color in the stream\n",
    "                            cv2.putText(frame, color_name, (int(xyxy[0]), int(xyxy[1]) + 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        0.9, color_labels[color], 2)\n",
    "\n",
    "                    # Detect face expression\n",
    "                    # Perform emotion analysis on the frame\n",
    "                    result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "                    # Retrieve the dominant emotion from the first result in the list\n",
    "                    dominant_emotion = result[0]['dominant_emotion']\n",
    "\n",
    "                    # Display the dominant emotion label on the frame\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        dominant_emotion,\n",
    "                        (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.9,\n",
    "                        (0, 255, 0),\n",
    "                        2\n",
    "                    )\n",
    "\n",
    "    # Show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # If the 'q' key is pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video stream and close any open windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af8b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define the lower and upper boundaries of the colors in the HSV color space\n",
    "# colors = {\n",
    "#    'blue': {\n",
    "#         'lower': np.array([90, 100, 100]),\n",
    "#         'upper': np.array([130, 255, 255])\n",
    "#     },\n",
    "#     'yellow': {\n",
    "#         'lower': np.array([20, 100, 100]),\n",
    "#         'upper': np.array([30, 255, 255])\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Color labels for drawing text on the frame\n",
    "# color_labels = {\n",
    "#     'blue': (255, 0, 0),\n",
    "#     'yellow': (0, 255, 255)\n",
    "# }\n",
    "\n",
    "# # Initialize the list of tracked points, the frame counter, and the coordinate deltas\n",
    "# pts = deque(maxlen=32)\n",
    "# counter = 0\n",
    "# (dX, dY) = (0, 0)\n",
    "# direction = \"\"\n",
    "# # Load the face cascade classifier\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # Load object detection model\n",
    "# weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "# imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "# # Set device and half precision\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# half = device.type != 'cpu'\n",
    "\n",
    "# # Load model\n",
    "# model = attempt_load(weights, map_location=device)\n",
    "# stride = int(model.stride.max())\n",
    "# imgsz = check_img_size(imgsz, s=stride)  # Check image size\n",
    "\n",
    "# if half:\n",
    "#     model.half()  # To FP16\n",
    "\n",
    "# names = model.module.names if hasattr(model, 'module') else model.names\n",
    "# color_list = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "# # Load face recognition model\n",
    "# Supra_image = face_recognition.load_image_file(\"Supraa.jpg\")\n",
    "# Supra_face_encoding = face_recognition.face_encodings(Supra_image)[0]\n",
    "# known_face_encodings = [Supra_face_encoding]  # Replace with your known face encodings\n",
    "# known_face_names = ['Supraa','Mortaga','Menna']  # Replace with your known face names\n",
    "\n",
    "# # Initialize the emotion analysis\n",
    "# gaze = GazeTracking()\n",
    "# video_capture = cv2.VideoCapture(0)  # 0 represents the default camera\n",
    "# #cv2.namedWindow(\"Real-time Emotion Analysis\")\n",
    "\n",
    "# # Allow the camera to warm up\n",
    "# time.sleep(2.0)\n",
    "# # Calculate the distance between two points\n",
    "# def calculate_distance(point1, point2):\n",
    "#     return np.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "# while True:\n",
    "#     # Read the next frame from the video stream\n",
    "#     ret, frame = video_capture.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     # Face detection\n",
    "#     small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "#     rgb_small_frame = small_frame[:, :, ::-1]\n",
    "#     face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "#     face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "#     face_names = []\n",
    "\n",
    "#     for face_encoding in face_encodings:\n",
    "#         matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "#         name = \"Unknown\"\n",
    "\n",
    "#         face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "#         best_match_index = np.argmin(face_distances)\n",
    "#         if matches[best_match_index]:\n",
    "#             name = known_face_names[best_match_index]\n",
    "\n",
    "#         face_names.append(name)\n",
    "\n",
    "#     # Display the Face Detection results\n",
    "#     for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "#         top *= 4\n",
    "#         right *= 4\n",
    "#         bottom *= 4\n",
    "#         left *= 4\n",
    "\n",
    "#         cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "#     gaze.refresh(frame)\n",
    "#     new_frame = gaze.annotated_frame()\n",
    "#     text = \"\"\n",
    "#     if gaze.is_right():\n",
    "#         text = \"Looking right\"\n",
    "#     elif gaze.is_left():\n",
    "#         text = \"Looking left\"\n",
    "#     elif gaze.is_center():\n",
    "#         text = \"Looking center\"\n",
    "#     elif gaze.is_blinking():\n",
    "#         text = \"Blinking\"\n",
    "\n",
    "#     cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "#     # Resize the frame, blur it, and convert it to the HSV color space\n",
    "#     frame = imutils.resize(frame, width=600)\n",
    "#     blurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "#     hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#     # Detect objects using YOLOv7\n",
    "#     img = letterbox(frame, imgsz, stride=stride)[0]\n",
    "#     img = img.transpose((2, 0, 1))[::-1]\n",
    "#     img = np.ascontiguousarray(img)\n",
    "\n",
    "#     img = torch.from_numpy(img).to(device)\n",
    "#     img = img.half() if half else img.float()\n",
    "#     img /= 255.0\n",
    "#     if img.ndimension() == 3:\n",
    "#         img = img.unsqueeze(0)\n",
    "\n",
    "#     # Inference\n",
    "#     t1 = time_synchronized()\n",
    "#     pred = model(img, augment=False)[0]\n",
    "#     pred = non_max_suppression(pred, 0.4, 0.5, classes=None, agnostic=False)\n",
    "#     t2 = time_synchronized()\n",
    "\n",
    "#     # Process detections\n",
    "#     for i, det in enumerate(pred):  # Detections per image\n",
    "#         if len(det):\n",
    "#             det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "\n",
    "#             # Iterate over detections and draw bounding boxes\n",
    "#             for *xyxy, conf, cls in reversed(det):\n",
    "#                 c = int(cls)  # Integer class\n",
    "#                 label = f'{names[c]} {conf:.2f}'\n",
    "#                 color = color_list[c]\n",
    "\n",
    "#                 # Draw bounding boxes\n",
    "#                 cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), color, 2)\n",
    "#                 cv2.putText(frame, label, (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "#                 # Convert the frame to grayscale and equalize the histogram\n",
    "#                 gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#                 gray = cv2.equalizeHist(gray)\n",
    "\n",
    "#                 # Detect faces in the grayscale frame\n",
    "#                 faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30),\n",
    "#                                                       flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "#                 # Iterate over the detected faces\n",
    "#                 for (x, y, w, h) in faces:\n",
    "#                     # Compute the center of the face\n",
    "#                     center = (x + w // 2, y + h // 2)\n",
    "\n",
    "#                     # Update the points queue\n",
    "#                     pts.appendleft(center)\n",
    "\n",
    "#                     # Draw a rectangle around the face\n",
    "#                     cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "#                     # Draw the center of the face\n",
    "#                     cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "\n",
    "#                     # Calculate the distance between the center of the face and the color\n",
    "#                     color_name = names[c]\n",
    "#                     color_center = (int((xyxy[0] + xyxy[2]) / 2), int((xyxy[1] + xyxy[3]) / 2))\n",
    "#                     for color, bounds in colors.items():\n",
    "#                         lower = bounds['lower']\n",
    "#                         upper = bounds['upper']\n",
    "#                         if lower[0] <= hsv[color_center[1], color_center[0], 0] <= upper[0]:\n",
    "#                             distance = calculate_distance(center, color_center)\n",
    "\n",
    "#                             # Draw a line between the face and the color\n",
    "#                             cv2.line(frame, center, color_center, color_labels[color], 2)\n",
    "\n",
    "#                             # Check if the distance is less than 100\n",
    "#                             if distance < 100:\n",
    "#                                 # Play a sound\n",
    "#                                 winsound.Beep(1000, 500)  # Beep with frequency 1000Hz for 500 milliseconds\n",
    "\n",
    "#                             # Label the color in the stream\n",
    "#                             cv2.putText(frame, color_name, (int(xyxy[0]), int(xyxy[1]) + 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                                         0.9, color_labels[color], 2)\n",
    "\n",
    "#                     # Detect face expression\n",
    "#                     # Perform emotion analysis on the frame\n",
    "#                     result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "#                     # Retrieve the dominant emotion from the first result in the list\n",
    "#                     dominant_emotion = result[0]['dominant_emotion']\n",
    "\n",
    "#                     # Display the dominant emotion label on the frame\n",
    "#                     cv2.putText(\n",
    "#                         frame,\n",
    "#                         dominant_emotion,\n",
    "#                         (x, y - 10),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                         0.9,\n",
    "#                         (0, 255, 0),\n",
    "#                         2\n",
    "#                     )\n",
    "\n",
    "#     # Show the output frame\n",
    "#     cv2.imshow(\"Frame\", frame)\n",
    "#     key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "#     # If the 'q' key is pressed, break from the loop\n",
    "#     if key == ord(\"q\"):\n",
    "#         break\n",
    "\n",
    "# # Release the video stream and close any open windows\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Video information\n",
    "# fps = 30  # Adjust the frame rate as needed\n",
    "# w, h = 640, 480  # Set the desired width and height for the video output\n",
    "\n",
    "# # Initializing video object\n",
    "# video = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "\n",
    "# # Initializing object for writing video output\n",
    "# #output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'), fps, (w, h))\n",
    "\n",
    "# # Initializing model and setting it for inference\n",
    "# with torch.no_grad():\n",
    "#     weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "#     imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "#     set_logging()\n",
    "#     device = select_device('')\n",
    "#     half = device.type != 'cpu'\n",
    "#     model = attempt_load(weights, map_location=device)\n",
    "#     stride = int(model.stride.max())\n",
    "#     imgsz = check_img_size(imgsz, s=stride)\n",
    "#     if half:\n",
    "#         model.half()\n",
    "\n",
    "#     names = model.module.names if hasattr(model, 'module') else model.names\n",
    "#     colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "#     if device.type != 'cpu':\n",
    "#         model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "#     while True:\n",
    "#         ret, img0 = video.read()\n",
    "\n",
    "#         if ret:\n",
    "#             img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "#             img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "#             img = np.ascontiguousarray(img)\n",
    "#             img = torch.from_numpy(img).to(device)\n",
    "#             img = img.half() if half else img.float()\n",
    "#             img /= 255.0\n",
    "#             if img.ndimension() == 3:\n",
    "#                 img = img.unsqueeze(0)\n",
    "\n",
    "#             # Inference\n",
    "#             t1 = time_synchronized()\n",
    "#             pred = model(img, augment=False)[0]\n",
    "#             pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.6)\n",
    "\n",
    "#             t2 = time_synchronized()\n",
    "#             for i, det in enumerate(pred):\n",
    "#                 if len(det):\n",
    "#                     det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "#                     for *xyxy, conf, cls in reversed(det):\n",
    "#                         label = f'{names[int(cls)]} {conf:.2f}'\n",
    "#                         plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "#             cv2.imshow('Object Detection', img0)\n",
    "#             #output.write(cv2.resize(img0, (w, h)))\n",
    "\n",
    "#             if cv2.waitKey(1) == ord('q'):  # Press 'q' to exit\n",
    "#                 break\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "# #output.release()\n",
    "# video.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize video capture object\n",
    "# gaze = GazeTracking()\n",
    "# video_capture = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "# # Load a sample picture and learn how to recognize it.\n",
    "# Supra_image = face_recognition.load_image_file(\"Supraa.jpg\")\n",
    "# Supra_face_encoding = face_recognition.face_encodings(Supra_image)[0]\n",
    "# # Load face recognition model and known face encodings\n",
    "# known_face_encodings = [Supra_face_encoding]  # Replace with your known face encodings\n",
    "# known_face_names = ['Supraa']  # Replace with your known face names\n",
    "\n",
    "# # Load object detection model\n",
    "# weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "# imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "# set_logging()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# half = device.type != 'cpu'\n",
    "# model = attempt_load(weights, map_location=device)\n",
    "# stride = int(model.stride.max())\n",
    "# imgsz = check_img_size(imgsz, s=stride)\n",
    "# if half:\n",
    "#     model.half()\n",
    "\n",
    "# names = model.module.names if hasattr(model, 'module') else model.names\n",
    "# colors = [[np.random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "# if device.type != 'cpu':\n",
    "#     model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "# # Process frames from the video capture\n",
    "# while True:\n",
    "#     # Grab a single frame of video\n",
    "#     ret, frame = video_capture.read()\n",
    "\n",
    "#     if ret:\n",
    "#         # Face detection\n",
    "#         small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "#         rgb_small_frame = small_frame[:, :, ::-1]\n",
    "#         face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "#         face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "#         gaze.refresh(frame)\n",
    "\n",
    "#         new_frame = gaze.annotated_frame()\n",
    "#         text = \"\"\n",
    "#         if gaze.is_right():\n",
    "#             text = \"Looking right\"\n",
    "#         elif gaze.is_left():\n",
    "#             text = \"Looking left\"\n",
    "#         elif gaze.is_center():\n",
    "#             text = \"Looking center\"\n",
    "#         elif gaze.is_blinking():\n",
    "#             text = \"Blinking\"\n",
    "            \n",
    "#         cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
    "#         print(text)\n",
    "#         face_names = []\n",
    "#         for face_encoding in face_encodings:\n",
    "#             matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "#             name = \"Unknown\"\n",
    "\n",
    "#             face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "#             best_match_index = np.argmin(face_distances)\n",
    "#             if matches[best_match_index]:\n",
    "#                 name = known_face_names[best_match_index]\n",
    "\n",
    "#             face_names.append(name)\n",
    "\n",
    "#         # Object detection\n",
    "#         img0 = frame\n",
    "#         img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "#         img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "#         img = np.ascontiguousarray(img)\n",
    "#         img = torch.from_numpy(img).to(device)\n",
    "#         img = img.half() if half else img.float()\n",
    "#         img /= 255.0\n",
    "#         if img.ndimension() == 3:\n",
    "#             img = img.unsqueeze(0)\n",
    "\n",
    "#         # Inference\n",
    "#         t1 = time_synchronized()\n",
    "#         pred = model(img, augment=False)[0]\n",
    "#         pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.6)\n",
    "#         t2 = time_synchronized()\n",
    "\n",
    "#         # Display the results\n",
    "#         for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "#             top *= 4\n",
    "#             right *= 4\n",
    "#             bottom *= 4\n",
    "#             left *= 4\n",
    "\n",
    "#             cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#             cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#             font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "#         for i, det in enumerate(pred):\n",
    "#             if len(det):\n",
    "#                 det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "#                 for *xyxy, conf, cls in reversed(det):\n",
    "#                     label = f'{names[int(cls)]} {conf:.2f}'\n",
    "#                     plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "#         cv2.imshow('Video', frame)\n",
    "#         if cv2.waitKey(1) == ord('q'):  # Press 'q' to exit\n",
    "#             break\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# # Release resources\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb1d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80485c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb517d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

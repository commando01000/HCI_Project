{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c69ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62c3fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "556578ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\HCI_Project_Phase_2\\yolov7\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\dell\\HCI_Project_Phase_2\\yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8244d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd C:\\Users\\dell\\HCI_Project_Phase_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b579c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e563ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu==2.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77ffe831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b7e4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7af8040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.10.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (66.0.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (3.19.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (23.5.9)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (16.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (0.31.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (23.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.24.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (2.10.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (1.54.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorflow==2.10.0) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.30.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.18.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.26.15)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (6.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dell\\anaconda3\\envs\\hcii\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0726d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.10.0\n",
      "TensorFlow-GPU enabled: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow-GPU enabled:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "334775ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11169218171271786995\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4163895296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6594695589651776879\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89b99ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # GPU(s) detected\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU:\", gpu)\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db63dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9010f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed2b820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "import face_recognition\n",
    "from gaze_tracking import GazeTracking\n",
    "import deepface\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dd33926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0069706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98dcd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_filter = ['train'] #You can give list of classes to filter by name ['train','person' ]\n",
    "\n",
    "\n",
    "opt  = {\n",
    "    \"weights\": \"weights/yolov7.pt\", # Path to weights file default weights are for nano model\n",
    "    \"yaml\"   : \"data/coco.yaml\",\n",
    "    \"img-size\": 640, # default image size\n",
    "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
    "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
    "    \"device\" : 'cpu',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
    "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "858d9b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  bf4d028 torch 2.0.1+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 306 layers, 36905341 parameters, 6652669 gradients\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Video information\n",
    "# fps = 30  # Adjust the frame rate as needed\n",
    "# w, h = 640, 480  # Set the desired width and height for the video output\n",
    "\n",
    "# # Initializing video object\n",
    "# video = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "\n",
    "# # Initializing object for writing video output\n",
    "# #output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'), fps, (w, h))\n",
    "\n",
    "# # Initializing model and setting it for inference\n",
    "# with torch.no_grad():\n",
    "#     weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "#     imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "#     set_logging()\n",
    "#     device = select_device('')\n",
    "#     half = device.type != 'cpu'\n",
    "#     model = attempt_load(weights, map_location=device)\n",
    "#     stride = int(model.stride.max())\n",
    "#     imgsz = check_img_size(imgsz, s=stride)\n",
    "#     if half:\n",
    "#         model.half()\n",
    "\n",
    "#     names = model.module.names if hasattr(model, 'module') else model.names\n",
    "#     colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "#     if device.type != 'cpu':\n",
    "#         model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "#     while True:\n",
    "#         ret, img0 = video.read()\n",
    "\n",
    "#         if ret:\n",
    "#             img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "#             img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "#             img = np.ascontiguousarray(img)\n",
    "#             img = torch.from_numpy(img).to(device)\n",
    "#             img = img.half() if half else img.float()\n",
    "#             img /= 255.0\n",
    "#             if img.ndimension() == 3:\n",
    "#                 img = img.unsqueeze(0)\n",
    "\n",
    "#             # Inference\n",
    "#             t1 = time_synchronized()\n",
    "#             pred = model(img, augment=False)[0]\n",
    "#             pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.6)\n",
    "\n",
    "#             t2 = time_synchronized()\n",
    "#             for i, det in enumerate(pred):\n",
    "#                 if len(det):\n",
    "#                     det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "#                     for *xyxy, conf, cls in reversed(det):\n",
    "#                         label = f'{names[int(cls)]} {conf:.2f}'\n",
    "#                         plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "#             cv2.imshow('Object Detection', img0)\n",
    "#             #output.write(cv2.resize(img0, (w, h)))\n",
    "\n",
    "#             if cv2.waitKey(1) == ord('q'):  # Press 'q' to exit\n",
    "#                 break\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "# #output.release()\n",
    "# video.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4679f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize video capture object\n",
    "# gaze = GazeTracking()\n",
    "# video_capture = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "# # Load a sample picture and learn how to recognize it.\n",
    "# Supra_image = face_recognition.load_image_file(\"Supraa.jpg\")\n",
    "# Supra_face_encoding = face_recognition.face_encodings(Supra_image)[0]\n",
    "# # Load face recognition model and known face encodings\n",
    "# known_face_encodings = [Supra_face_encoding]  # Replace with your known face encodings\n",
    "# known_face_names = ['Supraa']  # Replace with your known face names\n",
    "\n",
    "# # Load object detection model\n",
    "# weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "# imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "# set_logging()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# half = device.type != 'cpu'\n",
    "# model = attempt_load(weights, map_location=device)\n",
    "# stride = int(model.stride.max())\n",
    "# imgsz = check_img_size(imgsz, s=stride)\n",
    "# if half:\n",
    "#     model.half()\n",
    "\n",
    "# names = model.module.names if hasattr(model, 'module') else model.names\n",
    "# colors = [[np.random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "# if device.type != 'cpu':\n",
    "#     model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "# # Process frames from the video capture\n",
    "# while True:\n",
    "#     # Grab a single frame of video\n",
    "#     ret, frame = video_capture.read()\n",
    "\n",
    "#     if ret:\n",
    "#         # Face detection\n",
    "#         small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "#         rgb_small_frame = small_frame[:, :, ::-1]\n",
    "#         face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "#         face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "#         gaze.refresh(frame)\n",
    "\n",
    "#         new_frame = gaze.annotated_frame()\n",
    "#         text = \"\"\n",
    "#         if gaze.is_right():\n",
    "#             text = \"Looking right\"\n",
    "#         elif gaze.is_left():\n",
    "#             text = \"Looking left\"\n",
    "#         elif gaze.is_center():\n",
    "#             text = \"Looking center\"\n",
    "#         elif gaze.is_blinking():\n",
    "#             text = \"Blinking\"\n",
    "            \n",
    "#         cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
    "#         print(text)\n",
    "#         face_names = []\n",
    "#         for face_encoding in face_encodings:\n",
    "#             matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "#             name = \"Unknown\"\n",
    "\n",
    "#             face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "#             best_match_index = np.argmin(face_distances)\n",
    "#             if matches[best_match_index]:\n",
    "#                 name = known_face_names[best_match_index]\n",
    "\n",
    "#             face_names.append(name)\n",
    "\n",
    "#         # Object detection\n",
    "#         img0 = frame\n",
    "#         img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "#         img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "#         img = np.ascontiguousarray(img)\n",
    "#         img = torch.from_numpy(img).to(device)\n",
    "#         img = img.half() if half else img.float()\n",
    "#         img /= 255.0\n",
    "#         if img.ndimension() == 3:\n",
    "#             img = img.unsqueeze(0)\n",
    "\n",
    "#         # Inference\n",
    "#         t1 = time_synchronized()\n",
    "#         pred = model(img, augment=False)[0]\n",
    "#         pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.6)\n",
    "#         t2 = time_synchronized()\n",
    "\n",
    "#         # Display the results\n",
    "#         for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "#             top *= 4\n",
    "#             right *= 4\n",
    "#             bottom *= 4\n",
    "#             left *= 4\n",
    "\n",
    "#             cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#             cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#             font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "#         for i, det in enumerate(pred):\n",
    "#             if len(det):\n",
    "#                 det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "#                 for *xyxy, conf, cls in reversed(det):\n",
    "#                     label = f'{names[int(cls)]} {conf:.2f}'\n",
    "#                     plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "#         cv2.imshow('Video', frame)\n",
    "#         if cv2.waitKey(1) == ord('q'):  # Press 'q' to exit\n",
    "#             break\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# # Release resources\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3797b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  76a11f5 torch 2.0.1+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.11it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 15.92it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 15.72it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 14.24it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.56it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  9.91it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.44it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.85it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.30it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.22it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 18.38it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.23it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.00it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 18.58it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.78it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.07it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.54it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.46it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 14.81it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.80it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.48it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 17.11it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.71it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.76it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize video capture object\n",
    "gaze = GazeTracking()\n",
    "video_capture = cv2.VideoCapture(0)  # Use index 0 for the default camera\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "Supra_image = face_recognition.load_image_file(\"Supraa.jpg\")\n",
    "Supra_face_encoding = face_recognition.face_encodings(Supra_image)[0]\n",
    "# Load face recognition model and known face encodings\n",
    "known_face_encodings = [Supra_face_encoding]  # Replace with your known face encodings\n",
    "known_face_names = ['Supraa']  # Replace with your known face names\n",
    "\n",
    "# Load object detection model\n",
    "weights = 'weights/yolov7.pt'  # Replace with the path to your YOLOv7 weights\n",
    "imgsz = 416  # Set the input image size for the model\n",
    "\n",
    "set_logging()\n",
    "device = select_device('')\n",
    "half = device.type != 'cpu'\n",
    "model = attempt_load(weights, map_location=device)\n",
    "stride = int(model.stride.max())\n",
    "imgsz = check_img_size(imgsz, s=stride)\n",
    "if half:\n",
    "    model.half()\n",
    "\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
    "\n",
    "# Process frames from the video capture\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if ret:\n",
    "        # Face detection\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        gaze.refresh(frame)\n",
    "        new_frame = gaze.annotated_frame()\n",
    "        text = \"\"\n",
    "        if gaze.is_right():\n",
    "            text = \"Looking right\"\n",
    "        elif gaze.is_left():\n",
    "            text = \"Looking left\"\n",
    "        elif gaze.is_center():\n",
    "            text = \"Looking center\"\n",
    "        elif gaze.is_blinking():\n",
    "            text = \"Blinking\"\n",
    "\n",
    "        cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
    "        print(text)\n",
    "        \n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "        #Face Expressions\n",
    "        result = DeepFace.analyze(frame, actions=['emotion'],enforce_detection=False)\n",
    "        # Retrieve the dominant emotion from the first result in the list\n",
    "        dominant_emotion = result[0]['dominant_emotion']\n",
    "        # Display the frame with emotion labels\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            dominant_emotion,\n",
    "            (50, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        # Object detection\n",
    "        img0 = frame\n",
    "        img = letterbox(img0, imgsz, stride=stride)[0]\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()\n",
    "        img /= 255.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=False)[0]\n",
    "        pred = non_max_suppression(pred, conf_thres=0.3, iou_thres=0.6)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Display the results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "        for i, det in enumerate(pred):\n",
    "            if len(det):\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                    plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) == ord('q'):  # Press 'q' to exit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46147fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
